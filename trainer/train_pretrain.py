import os
import sys


__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import time  # æ—¶é—´ç»Ÿè®¡
import warnings  # è­¦å‘Šæ§åˆ¶
import torch  
import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
from contextlib import nullcontext  # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
from torch import optim, nn  # ä¼˜åŒ–å™¨å’Œç¥ç»ç½‘ç»œæ¨¡å—
from torch.nn.parallel import DistributedDataParallel  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
from torch.utils.data import DataLoader, DistributedSampler  # æ•°æ®åŠ è½½å™¨

from model.minimindModel import MindConfig 
from dataset.lm_dataset import PretrainDataset 
from trainer.trainer_utils import (  # è®­ç»ƒå·¥å…·å‡½æ•°
    get_lr,
    Logger,
    is_main_process,
    lm_checkpoint,
    init_distributed_mode,
    setup_seed,
    init_model,
    SkipBatchSampler,
)

# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œä¿æŒè¾“å‡ºæ¸…æ´
warnings.filterwarnings("ignore")


def train_epoch(epoch, loader, iters, start_step=0, wandb=None):
    loss_fct = nn.CrossEntropyLoss(reduction="none")
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

    # éå†æ•°æ®æ‰¹æ¬¡
    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):
        X = X.to(args.device)
        Y = Y.to(args.device)
        loss_mask = loss_mask.to(args.device)

        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)

        for param_group in optimizer.param_groups:
            param_group["lr"] = lr

        with autocast_ctx:
            # å‰å‘ä¼ æ’­
            res = model(X)

            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),  # [batch*seq, vocab_size]
                Y.view(-1),  # [batch*seq]
            ).view(Y.size())  # æ¢å¤ä¸º [batch_size, seq_len]

            loss = (loss * loss_mask).sum() / loss_mask.sum()

            loss+=res.aux_loss
            
            loss = loss / args.accumulation_steps

        scaler.scale(loss).backward()

        if (step + 1) % args.accumulation_steps == 0:
            # scaler.unscale_(): è¿˜åŸæ¢¯åº¦çš„çœŸå®å€¼
            scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

            # ğŸ“š ä¼˜åŒ–å™¨æ›´æ–°çŸ¥è¯†ç‚¹
            # scaler.step(): æ‰§è¡Œå‚æ•°æ›´æ–°
            # scaler.update(): æ›´æ–°scalerçš„ç¼©æ”¾å› å­
            scaler.step(optimizer)
            scaler.update()

            optimizer.zero_grad(set_to_none=True)

        if step % args.log_interval == 0 or step == iters - 1:
            spend_time = time.time() - start_time
            current_loss = loss.item() * args.accumulation_steps  # æ¢å¤çœŸå®æŸå¤±å€¼
            current_lr = optimizer.param_groups[-1]["lr"]  # å½“å‰å­¦ä¹ ç‡

            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60

            Logger(
                f"Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:"
            )

            # è®°å½•åˆ°å®éªŒè·Ÿè¸ªç³»ç»Ÿ
            if wandb:
                wandb.log(
                    {"loss": current_loss, "lr": current_lr, "epoch_Time": eta_min}
                )

        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

            # æ„å»ºä¿å­˜è·¯å¾„
            moe_suffix = (
                "_moe" if hasattr(lm_config, "use_moe") and lm_config.use_moe else ""
            )
            ckp = f"{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth"

            # ğŸ“š åˆ†å¸ƒå¼æ¨¡å‹ä¿å­˜çŸ¥è¯†ç‚¹
            # DDPæ¨¡å‹éœ€è¦é€šè¿‡.moduleè®¿é—®çœŸæ­£çš„æ¨¡å‹
            if isinstance(model, torch.nn.parallel.DistributedDataParallel):
                state_dict = model.module.state_dict()
            else:
                state_dict = model.state_dict()

            # ğŸ“š åŠç²¾åº¦ä¿å­˜çŸ¥è¯†ç‚¹
            # å°†float32å‚æ•°è½¬ä¸ºfloat16ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´
            state_dict = {k: v.half() for k, v in state_dict.items()}
            torch.save(state_dict, ckp)

            # ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€
            lm_checkpoint(
                lm_config,
                weight=args.save_weight,
                model=model,
                optimizer=optimizer,
                scaler=scaler,
                epoch=epoch,
                step=step,
                wandb=wandb,
                save_dir="checkpoints",
            )

            model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MiniMind Pretraining")

    # ========== åŸºç¡€è®­ç»ƒå‚æ•° ==========
    parser.add_argument("--save_dir", type=str, default="out", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument(
        "--save_weight", default="pretrain", type=str, help="ä¿å­˜æƒé‡çš„å‰ç¼€å"
    )
    parser.add_argument(
        "--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®1è½®zeroæˆ–2-6è½®å……åˆ†è®­ç»ƒï¼‰"
    )
    parser.add_argument("--batch_size", type=int, default=32, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="åˆå§‹å­¦ä¹ ç‡")

    # ========== ç¡¬ä»¶å’Œæ€§èƒ½å‚æ•° ==========
    parser.add_argument(
        "--device",
        type=str,
        default="cuda:0" if torch.cuda.is_available() else "cpu",
        help="è®­ç»ƒè®¾å¤‡",
    )
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    parser.add_argument("--num_workers", type=int, default=1, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")

    # ========== è®­ç»ƒç­–ç•¥å‚æ•° ==========
    parser.add_argument(
        "--accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°"
    )
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--log_interval", type=int, default=100, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=100, help="æ¨¡å‹ä¿å­˜é—´éš”")

    # ========== æ¨¡å‹æ¶æ„å‚æ•° ==========
    parser.add_argument("--hidden_size", default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_hidden_layers", default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument(
        "--max_seq_len", default=512, type=int, help="è®­ç»ƒçš„æœ€å¤§æˆªæ–­é•¿åº¦"
    )
    parser.add_argument(
        "--use_moe",
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )

    # ========== æ•°æ®å’Œæ¢å¤å‚æ•° ==========
    parser.add_argument(
        "--data_path",
        type=str,
        default="dataset/pretrain_hq.jsonl",
        help="é¢„è®­ç»ƒæ•°æ®è·¯å¾„",
    )
    parser.add_argument(
        "--from_weight",
        default="none",
        type=str,
        help="åŸºäºå“ªä¸ªæƒé‡è®­ç»ƒï¼Œä¸ºnoneåˆ™ä»å¤´å¼€å§‹",
    )
    parser.add_argument(
        "--from_resume",
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )

    # ========== å®éªŒè·Ÿè¸ªå‚æ•° ==========
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument(
        "--wandb_project", type=str, default="MiniMind-Pretrain", help="wandbé¡¹ç›®å"
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # ========== 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­ ==========
    """
    ğŸ“š åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–çŸ¥è¯†ç‚¹ï¼š
    - local_rank: å½“å‰è¿›ç¨‹åœ¨æœ¬æœºä¸Šçš„GPUç¼–å·
    - éšæœºç§å­: ç¡®ä¿ä¸åŒè¿›ç¨‹æœ‰ä¸åŒä½†å¯å¤ç°çš„éšæœºåºåˆ—
    - è¿™æ ·æ—¢ä¿è¯äº†éšæœºæ€§ï¼Œåˆä¿è¯äº†å¯å¤ç°æ€§
    """
    local_rank = init_distributed_mode()
    if dist.is_initialized():
        args.device = f"cuda:{local_rank}"  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä½¿ç”¨å¯¹åº”çš„GPU

    # ğŸ“š éšæœºç§å­è®¾ç½®çŸ¥è¯†ç‚¹
    # ä¸åŒè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„ç§å­ï¼Œé¿å…æ•°æ®é‡‡æ ·å®Œå…¨ç›¸åŒ
    # 42æ˜¯åŸºç¡€ç§å­ï¼Œæ¯ä¸ªè¿›ç¨‹åŠ ä¸Šè‡ªå·±çš„rankä¿è¯ä¸åŒ
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))

    # ========== 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ç‚¹ ==========
    """
    ğŸ“š æ¨¡å‹é…ç½®å’Œæ£€æŸ¥ç‚¹ç®¡ç†ï¼š
    - åˆ›å»ºä¿å­˜ç›®å½•
    - æ„å»ºæ¨¡å‹é…ç½®å¯¹è±¡
    - å°è¯•åŠ è½½æ–­ç‚¹ç»­è®­æ•°æ®
    """
    os.makedirs(args.save_dir, exist_ok=True)  # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨

    # åˆ›å»ºMiniMindæ¨¡å‹é…ç½®
    lm_config = MindConfig(
        hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers,use_moe=bool(args.use_moe)
    )

    # ğŸ“š æ–­ç‚¹ç»­è®­çŸ¥è¯†ç‚¹
    # å¦‚æœå¼€å¯äº†æ–­ç‚¹ç»­è®­ï¼Œå°è¯•åŠ è½½ä¹‹å‰çš„è®­ç»ƒçŠ¶æ€
    ckp_data = (
        lm_checkpoint(lm_config, weight=args.save_weight, save_dir="checkpoints")
        if args.from_resume == 1
        else None
    )

    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    """
    ğŸ“š æ··åˆç²¾åº¦è®­ç»ƒçŸ¥è¯†ç‚¹ï¼š
    - bfloat16: Googleå¼€å‘ï¼Œæ•°å€¼èŒƒå›´å¤§ï¼Œæ›´ç¨³å®š
    - float16: æ ‡å‡†åŠç²¾åº¦ï¼ŒèŠ‚çœå†…å­˜ä½†å¯èƒ½æº¢å‡º
    - autocast: è‡ªåŠ¨é€‰æ‹©ç²¾åº¦ï¼Œå…³é”®è¿ç®—ç”¨float32
    """
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16

    # ğŸ“š ä¸Šä¸‹æ–‡ç®¡ç†å™¨çŸ¥è¯†ç‚¹
    # CPUä¸æ”¯æŒautocastï¼Œä½¿ç”¨nullcontextä½œä¸ºç©ºæ“ä½œ
    autocast_ctx = (
        nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)
    )

    # ========== 4. é…ç½®WandBå®éªŒè·Ÿè¸ª ==========
    """
    ğŸ“š å®éªŒè·Ÿè¸ªç³»ç»ŸçŸ¥è¯†ç‚¹ï¼š
    - WandB: å®éªŒç®¡ç†å¹³å°ï¼Œè®°å½•è®­ç»ƒè¿‡ç¨‹
    - SwanLab: å›½äº§æ›¿ä»£æ–¹æ¡ˆ
    - æ”¯æŒæ–­ç‚¹ç»­è®­æ—¶æ¢å¤åˆ°åŒä¸€ä¸ªå®éªŒ
    """
    wandb = None
    if args.use_wandb and is_main_process():
        # ä½¿ç”¨SwanLabä½œä¸ºWandBçš„æ›¿ä»£
        import swanlab as wandb

        # ğŸ“š å®éªŒæ¢å¤çŸ¥è¯†ç‚¹
        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹æ•°æ®ï¼Œè·å–ä¹‹å‰çš„wandb_idæ¥æ¢å¤å®éªŒ
        wandb_id = ckp_data.get("wandb_id") if ckp_data else None
        resume = "must" if wandb_id else None  # å¿…é¡»æ¢å¤åˆ°æŒ‡å®šå®éªŒ

        # æ„å»ºå®éªŒåç§°ï¼ŒåŒ…å«å…³é”®è¶…å‚æ•°
        wandb_run_name = f"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}"
        wandb.init(
            project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume
        )

    # ========== 5. å®šä¹‰æ¨¡å‹ã€æ•°æ®ã€ä¼˜åŒ–å™¨ ==========
    """
    ğŸ“š è®­ç»ƒç»„ä»¶åˆå§‹åŒ–ï¼š
    - æ¨¡å‹: æ ¹æ®é…ç½®åˆ›å»ºMiniMindæ¨¡å‹
    - æ•°æ®é›†: åŠ è½½é¢„è®­ç»ƒæ•°æ®
    - é‡‡æ ·å™¨: åˆ†å¸ƒå¼è®­ç»ƒçš„æ•°æ®åˆ†é…
    - ä¼˜åŒ–å™¨: AdamWä¼˜åŒ–å™¨
    - ç¼©æ”¾å™¨: æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾
    """
    # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)

    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=args.max_seq_len)

    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None

    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == "float16"))

    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)

    start_epoch, start_step = 0, 0
    if ckp_data:
        # æ¢å¤æ¨¡å‹å‚æ•°
        model.load_state_dict(ckp_data["model"])
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŠ¨é‡ã€æ–¹å·®ä¼°è®¡ç­‰ï¼‰
        optimizer.load_state_dict(ckp_data["optimizer"])
        # æ¢å¤æ¢¯åº¦ç¼©æ”¾å™¨çŠ¶æ€
        scaler.load_state_dict(ckp_data["scaler"])
        # æ¢å¤è®­ç»ƒè¿›åº¦
        start_epoch = ckp_data["epoch"]
        start_step = ckp_data.get("step", 0)

    if dist.is_initialized():
        # ğŸ“š RoPEä½ç½®ç¼–ç ç‰¹æ®Šå¤„ç†
        # freqs_cos, freqs_sinæ˜¯ä½ç½®ç¼–ç ç¼“å­˜ï¼Œä¸éœ€è¦æ¢¯åº¦åŒæ­¥
        model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        model = DistributedDataParallel(model, device_ids=[local_rank])

    for epoch in range(start_epoch, args.epochs):
        # ğŸ“š åˆ†å¸ƒå¼é‡‡æ ·å™¨epochè®¾ç½®
        # æ¯ä¸ªepochè®¾ç½®ä¸åŒçš„éšæœºç§å­ï¼Œç¡®ä¿æ•°æ®é¡ºåºéšæœºåŒ–
        if train_sampler:
            train_sampler.set_epoch(epoch)

        # ğŸ“š æ–­ç‚¹ç»­è®­é€»è¾‘
        if epoch == start_epoch and start_step > 0:  # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            # ä½¿ç”¨è·³æ‰¹é‡‡æ ·å™¨ï¼Œè·³è¿‡å·²è®­ç»ƒçš„æ•°æ®
            batch_sampler = SkipBatchSampler(
                train_sampler or range(len(train_ds)), args.batch_size, start_step + 1
            )
            loader = DataLoader(
                train_ds,
                batch_sampler=batch_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            Logger(
                f"Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹"
            )
            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)
        else:  # é»˜è®¤ä»å¤´å¼€å§‹
            loader = DataLoader(
                train_ds,
                batch_size=args.batch_size,
                shuffle=(train_sampler is None),
                sampler=train_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            train_epoch(epoch, loader, len(loader), 0, wandb)